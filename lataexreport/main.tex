\documentclass[twoside,a4paper,12pt]{report}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage{nameref}
\usepackage{float}
\usepackage{flafter}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\Urlmuskip=0mu plus 1mu
\usepackage[
backend=biber,
style=alphabetic
]{biblatex}
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}
 \usepackage[T1]{fontenc} 
 \usepackage[final]{microtype}
\emergencystretch=1em
\graphicspath{ {images/} }
\usepackage[utf8]{inputenc}
\usepackage[spanish,english]{babel}

\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor


\usepackage{titlesec}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
 
\setlength{\parindent}{4em}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1.3}

\usepackage{indentfirst}

\usepackage{mathptmx}


\usepackage{listings}
\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}



\graphicspath{ {images/} }

\usepackage[]{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 inner=25mm,
 outer=25mm,
 top=25mm,
 bottom=25mm
 }
 
 



\usepackage{csquotes}

\addbibresource{references.bib}

\usepackage{titlesec}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
 
\setlength{\parindent}{4em}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1.3}

\usepackage{indentfirst}

\usepackage[]{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 inner=25mm,
 outer=25mm,
 top=25mm,
 bottom=25mm
 }
 
% \title{Machine learning and pattern recognition project}
\usepackage{csquotes}
\begin{document}

\tableofcontents

\newpage


\setcounter{chapter}{1}


\section{Dataset}

The dataset contains the information of red and white variants of the Portuguese "Vinho Verde" wine. 
It has been split into Test and Train sets containing 1822 and 1839 samples respectively. 
Moreover, there are two classes: good quality (value 1) and bad quality wine (value 0), 
where each has twelve attributes. Moreover, the table \ref{RangeAndTypesAttributes} 
resumes the information related to the attributes.

\begin{enumerate}
    \item fixed acidity
    \item volatile acidity
    \item citric acid
    \item residual sugar
    \item chlorides
    \item free sulfur dioxide
    \item total sulfur dioxide
    \item density
    \item pH
    \item sulphates
    \item alcohol
\end{enumerate}



\begin{table}
\centering
 \begin{tabular}{||c c c||} 
 \hline \hline
 Attributes & Range & Type\\ 
 \hline\hline
 Fixed acidity & [3 ; 15.9] & Continuous\\ 
 \hline
 Volatile acidity & [0.1 ; 1.58] & Continuous\\
 \hline
 Citric acid & [0 ; 1] & Continuous \\
  \hline
 Residual sugar & [0.7 ; 23.5] & Continuous \\
  \hline
 Citric acid & [0 ; 1] & Continuous \\
 \hline
 Chlorides & [0.013 ; 0.611] & Continuous \\
 \hline
 Free sulfur dioxide & [2 ; 289] & Continuous \\
 \hline
 Total sulfur dioxide & [7 ; 440] & Continuous \\
  \hline
 Density & [0.9874 ; 1.0032] & Continuous \\
  \hline
  pH & [2.79 ; 3.9 ] & Continuous \\
   \hline
 Sulphates & [0.25 ; 1.36] & Continuous \\
  \hline
 Alcohol & [8 ; 14.9] & Continuous \\
  \hline
 Total sulfur dioxide & [7,440] & Continuous \\
 \hline \hline
\end{tabular}
\caption{Range and types of the attributes \label{RangeAndTypesAttributes}
}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth,height=0.7\textheight]{histograms.png}
    \caption{Histograms of the attributes (numerated) without modification}
     \label{HistogramsWithoutMod} 
\end{figure}

\subsection{Correlations}

The image \ref{CorrelationRaw} illustrates the correlation between the attributes in the dataset. 
It can be extracted from the figure that the most correlated attributes are (7,10), (6,5) (6,3) and 
(7,3) (see table \ref{ResumeCorrelation}).  Even though the correlation is higher in those attributes,
 the values are still unimportant (lower than 0.8) in order to consider a strong correlation between them.
  CONTINUE ANALYSISSSSSSS



\begin{figure}
    \centering
    \includegraphics[width=1\textwidth,height=0.5\textheight]{correlation_no_transformation.png}
    \caption{Correlations of the raw data (Pearson's coefficient)}
     \label{CorrelationRaw} 
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{|| c|c|c||}
          \hline \hline
         Attributes (by pairs) & Tag & Correlations  \\
         \hline \hline
         (7,10) & (density, alcohol) & -0.692\\
         \hline
         (6,5) & (total sulfur dioxide, free sulfur dioxide) & 0.73  \\
         \hline
         (6,3) & (total sulfur dioxide, residual sugar) & 0.52 \\
         \hline
         (7,3) & (density, residual sugar) &0.526 \\
         \hline  \hline
    \end{tabular}
    \caption{Most correlated attributes     \label{ResumeCorrelation}
    }
\end{table}




\subsection{PCA - Principal Component Analysis}

Principal Component Analysis or PCA is a widely used technique for dimensionality 
reduction of the large data set. Reducing the number of components or features costs
 some accuracy and on the other hand, it makes the large data set simpler, easy to 
 explore and visualize. Also, it reduces the computational complexity of the model 
 which makes machine learning algorithms run faster.

% \subsection{Use PCA on our dataset}
\textbf{Steps Involved in PCA}
\begin{enumerate}
    \item Standardize the data (with mean =0 and variance = 1).
    \item Compute the Covariance matrix of dimensions.
    \item Obtain the Eigenvectors and Eigenvalues from the covariance matrix
    \item Sort eigenvalues in descending order and choose the top k Eigenvectors 
    that correspond to the k largest eigenvalues (k will become the number of 
    dimensions of the new feature subspace k $\leq$ d, d being the number of original dimensions).
    \item Construct the projection matrix W from the selected k Eigenvectors.
    \item Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.
\end{enumerate}

Analyzing the eigenvalues obtained,  we can see how almost all of the information
can be obtained using 4 or 5 dimensions.
By drawing the cumulative graph of the eigenvalues, the same conclusion can be extracted.
 -insert plot cumulative explained variance.png-
Hence, PCA could be implemented and see if the accuracy improves significantly.


\section{Classifiers}
In order to perform the classification, we have performedd cross validation on the 
training set with k = 8.
The decision for that value of k was taken due to one major factor: time.In fact, the size of the sample being 
1839, the leave-one-out method was discarded due to time needed to perform it.
Moreover, while performing k-fold with some classifiers, k = 8 appeared as an affordable value 
concerning time consumption, whilst performing a decent classification (> 80\%).
Hence every time the k-fold is performed, we are using k = 8.

\subsection{Generative models}

In order to classify the data, different genereative models have been taken into account:
mutlivariate gaussian classifier, tied covariance, Naive Bayes through a k-fold approach including a PCA approach.
 The results are shown in the table \ref{diffTypesclass}. In order to compute the values, we calculated t
 
 The precision of the models seems to be similar, however the Multivariate  model has a
 greater precision so far with 81.4 \% with PCA, and Naive Bayes performs the worst with 76 \% (without PCA). 
 The explanation of the sligh worse performance of Naive Bayes  can be due to the fact that when we 
 perform the diagonalization of the covariances, we do so by putting to zero the other elements
  of the matrix, hence inevitably losing information. 

 So far, from the table it can be taken that the MVG classifier with PCA is so far the best solution with
  a 81.4 \% accuracy.

\begin{table}[H]
\centering
 \begin{tabular}{||c c c||} 
    \hline \hline
    Classifier & Accuracy (\%) & Accuracy with PCA (\%)\\
    \hline\hline
    Multivariate & 80.3 & \cellcolor{blue!25} 81.4  \\ 
    \hline
    Tied Covariance & 79 & 79.7  \\
    \hline
    Naive Bayes & 76 & 79.7 \\
    \hline \hline
\end{tabular}
\caption{Different types of classifiers and their accuracy\label{diffTypesclass}
}
\end{table}


\subsection{Logistic regression}
For the logistic regression model we used  a k-fold approach (k =8), where in each 
one we changed the value of lambda. The values of lambda tested where: 
[1e-05, 0.0001, 0.0002, 0.0004, 0.0006, 0.001, 0.004, 0.01]. 
This was done in order to try to get the best model possible.
 This approach was performed with PCA too.
The results are shown in table \ref{errorRateslinearregression}.


As the results in the table \ref{errorRateslinearregression}, the linear regression
without PCa, performed overall better than the model with the PCA.
The best performance of the PCA model was with lambda = 0.0004 
resulting in an error rate of 16.6. The model without PCA, outperforms the model 
with PCA, with its best error rate being 14.4\% (see value in blue).

\begin{table}
    \centering
     \begin{tabular}{||c c c ||} 
        \hline \hline
        lambda & PCA error rate (\%) & no PCA error rate(\%)\\
        \hline\hline
        1.e-5  & 18.34  & 18.8   \\ 
        \hline
        0.0001 & 19.21  &  17.5  \\ 
        \hline
        0.0002 & 20.1  &  14.8  \\ 
        \hline
        0.0004 & 16.6  &  \cellcolor{blue!25}  14.4  \\ 
        \hline
        0.006  & 24.9  & 21    \\ 
        \hline
        0.001  & 22.3  &  18.3  \\ 
        \hline
        0.004 & 20.5  &  20.5  \\ 
        \hline
        0.01  &  18.8 & 15.7   \\ 
        \hline \hline
    \end{tabular}
    \caption{Error rates in (\%) for linear regression with (and without) PCA     \label{errorRateslinearregression}
    }
    \end{table}


\subsection{Confusion matrix}
Concerning the confusion matrix classification model, it has also been performed 
using PCA and without PCA with a k-fold = 8.
We have also take into account a prior of 0.5 for each class. By trying other priors
the precision diminished. 
Additionally, we decided to put a cost for a false positive of 2 (CFP = 2), while a cost
for a false negative (CFN = 1). This decision was taken as we considered
that missclassifying a "bad" quality wine is worse than missclassifying a 
good quality wine.


The results of computing the optimal Bayes decisions for the parameters previously chosen,
are in the table \ref{errorRatesConfusionMatrix}. As it can be extracted from the table,
using PCA results in a worse performance the raw data inn every classifier except for the
Naive Bayes (with PCA 80.8 \% of precision versus 77.91 \%). 
Additionally, the best model seems to be MVG with a precision of 85.5\% and a DCF normalized
of 0.432 (which is the best among the 3 models).


\begin{table}[H]
    \centering
     \begin{tabular}{||c c c c||} 
        \hline \hline
        Classifier & Tied & MVG  & Naive \\
        \hline\hline
        Precision (\%) &  81.86 & \cellcolor{blue!25}  85.5 & 77.91  \\ 
        \hline
        Precision (PCA)  &  79.2 &  80.55 & 80.8 \\ 
        \hline
        DCF &  0.274 &  0.216 & 0.286  \\   
        \hline
        DCF (PCA)  &  0.312 &  0.291 & 0.33  \\   
        \hline
        DCF normalized &  0.548 &\cellcolor{blue!25}  0.432 & 0.572  \\ 
        \hline
        DCF normalized (PCA)  &  0.624 &  0.582 & 0.662    \\ 
        \hline \hline
    \end{tabular}
    
    \caption{Precision in (\%) for different logistic regression models \label{errorRatesConfusionMatrix}}
\end{table}


\subsection{SVM}

We have performed the linear SVM with raw data and with PCA. The results
with PCA, underperform the ones with raw data: the best error rate with PCA being
17.9 \% and without PCA 14.4 \% (check results in tables \ref{errorRatesLinearSVM} and \ref{errorRatesLinearSVMNoPCA} respectively).

However, the duality gap for the 17.9 \% error rate for PCA data is 119.13. This
value stands out among the others as really elevated . This could make us think 
that the reliability of the error rate obtained for c = 0.5 can be questioned.

On the other side, the raw data performed well for c = 0.04 and c 0.05
with both error rates being equal to 14.4 \% and the duality gap being 0
 (check table \ref{errorRatesLinearSVMNoPCA} for results).

\begin{table}[H]
    \centering
     \begin{tabular}{||c c c c||} 
        \hline \hline
            	 c 	& error rate &	 duality gap &	 primal \\ 
                 \hline
            	0.001	&   29.3 &	 0.0 	& 1.122 \\
                \hline
            	0.005 &   24.5 	& 0.0 &	 5.938 \\
                \hline
            	0.01	&   26.2 	 &0.0884 	& 13.15 \\
                \hline
            	0.05	&   30.1 	& 43.1	& 110.63 \\
                \hline
            	0.08	&   25.3 &	 6.11 &	 119.76 \\
                \hline
            	0.1	    &   23.14 	& 3.78	& 157.19 \\
                \hline
            	0.5	&    \cellcolor{blue!25} 17.9	& \cellcolor{red!25} 119.13	& 859.1 \\
                \hline
             	0.9	&   27.1 	& 342.73 	& 1057.13 \\
        \hline \hline
    \end{tabular}
    \caption{Error rate in (\%) for different parameters in linear SVM with PCA \label{errorRatesLinearSVM}}
\end{table}    


\begin{table}[H]
    \centering
     \begin{tabular}{||c c c ||} 
        \hline \hline
            c& error Rate& duality gap \\
            \hline \hline
            0.001&29.7& 0 \\
            \hline
            0.004&20.5& 0.0056\\
            \hline
            0.005& 21.8& 0\\
            \hline
            0.006& 20.1& 0\\
            \hline
            0.007& 16.6& 0\\
            \hline
            0.008& 27.5& 0.0013\\
            \hline
            0.009& 20.5& 0.0001\\
            \hline
            0.01& 17.5& 0\\
            \hline
            0.02&19.2& 0\\
            \hline
            0.03& 17.9& 0.0295\\
            \hline
            0.04& \cellcolor{blue!25}14.4& 0.014\\
            \hline
            0.05& \cellcolor{blue!25}14.4& 0\\
            \hline
            0.06& 21& 5.095\\
            \hline
            0.08& 24.9& 1.645\\
            \hline
            0.1& 19.2& 0.586\\
            \hline
            0.5& 31.9& 2259.61\\
            \hline
            0.9& 18.8& 97\\
\hline \hline
    \end{tabular}
    \caption{Error rate in (\%) for different parameters in linear SVM without PCA \label{errorRatesLinearSVMNoPCA}}
\end{table}    






\subsection{Gaussian mixture models}
For this classsifiers we decided to choose as parameters alpha = 0.1, minimum value
for the eigen vector 0.1. Moreover, we tested until gmm = 16 (1,2,4,8 and 16).

The choice for this parameters was based on the laboratory 10. Once the results appeared,
we didn't change the parameters due to the fact that the results were satisfying (see table \ref{errorRatesGMMs})
and that the computation for every case was too big to repeat. Hence, having
decent results in precision we estimated that the the recomputation was not worth the 
possibility of improvement.

The results shown in table \ref{errorRatesGMMs}, illustrate that there is a tendency 
when increasing the model's precision by augmenting the gmm (gmm =16 is the most precise model
in every scenario). On one hand, the most precise model is the full covariance classifier with PCA previously computed,
resulting in a 9.31\% error rate, versus 9.71 \% without PCA. On the other hand, with a lower complexity (gmm = 8),
the precision is still good enough for the full covariance without PCA (11.8 \%).


\begin{table}[H]
    \centering
     \begin{tabular}{||c c c c c c c c||} 
        \hline \hline
        Classifier & gmm = 1 & gmm = 2 & gmm = 4 & gmm = 8 &gmm = 16 & PCA & k-fold\\
        \hline\hline
        Full covariance &  16.4 &  15.7 &  15.4 & \cellcolor{blue!25}  11.8 & \cellcolor{blue!25} 9.71 & No & 8  \\ 
        \hline
        Full covariance &  17 &  15.8 &  15.8 &  12.9 & \cellcolor{blue!25} 9.31 & Yes & 4  \\ 
        \hline
        Tied Covariance &  16.4 &  16.4 & 16.4 &  16.4 & 14.9  & No & 8  \\
        \hline
        Tied Covariance &  18.6 &  18.6 & 19 &  18.4 & 17.6  & Yes & 8  \\
        \hline
        Diagonal covariance  &  22 &  21.6 & 19.2 &  15.1 & 13.5  & No & 8 \\
        \hline
        Diagonal covariance &20.3 &  21.3 & 21.3&  16.8  & 16 & Yes & 8\\
        \hline \hline
    \end{tabular}
    \caption{Error rates in (\%) for different GMM model classification    \label{errorRatesGMMs}    }
\end{table}

    

\section{Model choice and results}
With all the previously models taken into account, what appears to be best at
classifying the data is the GMM model, specifically for gmm =16 with a full covariance
and PCA applied:
\end{document}
